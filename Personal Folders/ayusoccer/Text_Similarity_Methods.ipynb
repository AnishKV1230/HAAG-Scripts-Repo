{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine Similarity"
      ],
      "metadata": {
        "id": "7Uam1dGHoGAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways to calculate the similarity between two blocks of text in Python. A common method is to use cosine similarity, which measures the cosine of the angle between two vectors in a multidimensional space. The code below will print the cosine similarity score between the two input text blocks, which ranges from 0 to 1. A score closer to 1 indicates higher similarity."
      ],
      "metadata": {
        "id": "q7Z5no92n8LG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbcuUJKlmuRi",
        "outputId": "4b18441d-1a14-41e9-c5a4-813dc7c3d4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score: 0.6201272584968651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "\n",
        "# Download the stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Function to calculate cosine similarity between two texts\n",
        "def calculate_similarity(text1, text2):\n",
        "    # Preprocess the texts\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "\n",
        "    # Create the TfidfVectorizer object\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Transform the texts to tf-idf vectors\n",
        "    vectors = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_matrix = cosine_similarity(vectors)\n",
        "    similarity_score = similarity_matrix[0][1]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example texts\n",
        "text1 = \"Natural language processing makes it possible for computers to understand human language.\"\n",
        "text2 = \"Computers are able to comprehend human language through natural language processing.\"\n",
        "\n",
        "# Calculate similarity\n",
        "similarity = calculate_similarity(text1, text2)\n",
        "print(f\"Similarity score: {similarity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jaccard Similarity"
      ],
      "metadata": {
        "id": "r0cvANFyoeqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download the stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Function to calculate Jaccard similarity between two texts\n",
        "def calculate_jaccard_similarity(text1, text2):\n",
        "    # Preprocess the texts\n",
        "    words1 = preprocess_text(text1)\n",
        "    words2 = preprocess_text(text2)\n",
        "\n",
        "    # Convert the lists of words to sets\n",
        "    set1 = set(words1)\n",
        "    set2 = set(words2)\n",
        "\n",
        "    # Calculate intersection and union\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "\n",
        "    # Calculate Jaccard similarity\n",
        "    jaccard_similarity = len(intersection) / len(union)\n",
        "\n",
        "    return jaccard_similarity\n",
        "\n",
        "# Example texts\n",
        "text1 = \"Natural language processing makes it possible for computers to understand human language.\"\n",
        "text2 = \"Computers are able to comprehend human language through natural language processing.\"\n",
        "\n",
        "# Calculate similarity\n",
        "similarity = calculate_jaccard_similarity(text1, text2)\n",
        "print(f\"Jaccard Similarity score: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8xG8te7of_b",
        "outputId": "23ff62de-2618-4c76-d2cb-61d15241fd7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity score: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Embeddings"
      ],
      "metadata": {
        "id": "6lG-vcBPoxOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download the stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Example texts\n",
        "text1 = \"Natural language processing makes it possible for computers to understand human language.\"\n",
        "text2 = \"Computers are able to comprehend human language through natural language processing.\"\n",
        "\n",
        "# Preprocess the texts\n",
        "documents = [preprocess_text(text1), preprocess_text(text2)]\n",
        "\n",
        "# Create TaggedDocument objects for training\n",
        "tagged_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
        "\n",
        "# Train a Doc2Vec model\n",
        "model = Doc2Vec(tagged_documents, vector_size=50, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Infer vectors for the documents\n",
        "vector1 = model.infer_vector(preprocess_text(text1))\n",
        "vector2 = model.infer_vector(preprocess_text(text2))\n",
        "\n",
        "# Calculate cosine similarity\n",
        "#similarity = calculate_similarity(vector1, vector2)\n",
        "# Create the TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Transform the texts to tf-idf vectors\n",
        "vectors = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "similarity = similarity_matrix[0][1]\n",
        "\n",
        "print(f\"Doc2Vec Similarity score: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6JB6a6mozEk",
        "outputId": "007a46ed-f065-48f1-964b-261b4a7df4dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc2Vec Similarity score: 0.5038711573210972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aside: Convert PDF to Text to Extract Resume Text"
      ],
      "metadata": {
        "id": "r9V7pH4mpbhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code block will be a useful starting point to convert resume PDFs to text."
      ],
      "metadata": {
        "id": "QGinngGFphTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyLFzX_oqHC_",
        "outputId": "5630a646-1a3d-4a79-9b29-90b50150464d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.4 PyMuPDFb-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Function to convert PDF to text\n",
        "def pdf_to_text(pdf_path):\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "\n",
        "    # Iterate over each page and extract text\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage with my resume\n",
        "pdf_path = '/content/Parikh_Ayush_2024_Resume.pdf'  # Path to your PDF file\n",
        "text = pdf_to_text(pdf_path)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "PyzTuiQVphwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}